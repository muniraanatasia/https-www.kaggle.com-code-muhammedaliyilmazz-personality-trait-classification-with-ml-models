# -*- coding: utf-8 -*-
"""Personality Trait Classification with ML Models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/muniraanatasiamhata/personality-trait-classification-with-ml-models.1ff9f1d5-5ab5-4396-8853-fd9b6aecc838.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250630/auto/storage/goog4_request%26X-Goog-Date%3D20250630T021418Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D39481e0237a8a4ccbb1ffdcf6ea77751949df25e6614e0e9fced7e7f3b37a60d0e9570850a0c07a41a49cc72efc777a6bee99d5c5cacc9c3f3016d7720c773dfb1a7fb953ffbef8c353b22ace72f16d1b83d184efb1ad8d0c7aff29a107b6399791e84fcfeea5e5c82a065e422c4ce1380017e952f9c0f38193bb9f08ae85c478590e112aa1dcc3e05d66e7dfc3d86535adfa56cdd47fd7b0887010a9d766da91d269a97c65882eda0868a1b1b7f48ceb7326ec54f495f31fad27739978e9cd8e243cd914608876dde63fb6ae0799bef9c305862c240729e905d629d40d408ad220fb3ce1b2e8cb993611f99e38b1a70b1f0e6c4438035266baa668c8697655e
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
syncoraai_extrovert_vs_introvert_personality_traits_dataset_path = kagglehub.dataset_download('syncoraai/extrovert-vs-introvert-personality-traits-dataset')

print('Data source import complete.')

"""---

## Dataset Summary

**Dataset Name:** Extrovert vs. Introvert Personality Traits Dataset (Synthetic)
**Source:** Generated by Syncoraai (synthetic behavioral data)
**Total Records:** 4,998
**Total Columns:** 8
**Task Type:** Binary Classification
**Target Variable:** `Personality`
**Class Labels:**

* `0` â†’ Extrovert
* `1` â†’ Introvert

The dataset contains information about individualsâ€™ behavioral patterns, habits, and social tendencies. Each row represents one person, and the goal is to classify them as either extroverted or introverted based on observed traits.

---

## Feature Descriptions

| Feature Name                | Data Type | Description                                                                                                           |
| --------------------------- | --------- | --------------------------------------------------------------------------------------------------------------------- |
| `Time_spent_Alone`          | float     | The number of hours a person spends alone per day. Higher values may indicate introverted tendencies.                 |
| `Stage_fear`                | int (0/1) | Indicates whether the person has a fear of public speaking. `1` means they do, which may correlate with introversion. |
| `Social_event_attendance`   | float     | Frequency of attending social events (e.g., parties, gatherings). Higher frequency suggests extroversion.             |
| `Going_outside`             | float     | Represents how often the person goes outdoors for activities or errands.                                              |
| `Drained_after_socializing` | int (0/1) | Whether the person feels emotionally drained after social interaction. `1` = Yes, common in introverts.               |
| `Friends_circle_size`       | float     | Size of the individualâ€™s social circle. A larger number typically correlates with extroversion.                       |
| `Post_frequency`            | float     | Frequency of posting on social media platforms. This can reflect communication preferences and sociability.           |
| `Personality`               | int (0/1) | **Target column.** `0` for Extrovert, `1` for Introvert. This is what we aim to predict.                              |

---

# STEP 1: Import Libraries
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
import warnings
warnings.filterwarnings("ignore")
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# STEP 2: Load the Dataset"""

df = pd.read_csv("/kaggle/input/extrovert-vs-introvert-personality-traits-dataset/Extrovert vs. Introvert Personality Traits Dataset_Syncoraai_Synthetic data.csv")

df.head()

df.tail()

df.info()

df.shape

df.describe()

df.nunique()

"""# STEP 3: Data Cleaning"""

df.isnull().sum()

print(f"Duplicates: {df.duplicated().sum()}")

# Remove negative zeros (if any)
df['Time_spent_Alone'] = df['Time_spent_Alone'].apply(lambda x: 0 if x == -0 else x)

"""# STEP 4: Exploratory Data Analysis (EDA)"""

# Ensure Personality is a category
df['Personality'] = df['Personality'].astype("category")

# 1. Violin plots for feature distribution by Personality
plt.figure(figsize=(18, 12))
for i, col in enumerate(df.drop("Personality", axis=1).columns, 1):
    plt.subplot(3, 3, i)
    sns.violinplot(x="Personality", y=col, data=df, palette="Set3", inner="quartile")
    plt.title(f"{col} Distribution by Personality")
plt.tight_layout()
plt.suptitle("Violin Plots of Features by Personality", y=1.02)
plt.show()

# 2. KDE plots: Density Estimation
plt.figure(figsize=(18, 12))
for i, col in enumerate(df.drop("Personality", axis=1).columns, 1):
    plt.subplot(3, 3, i)
    sns.kdeplot(data=df, x=col, hue="Personality", fill=True, alpha=0.5)
    plt.title(f"KDE Plot: {col}")
plt.tight_layout()
plt.suptitle("KDE Distributions by Personality", y=1.02)
plt.show()

# 3. Correlation Heatmap (Advanced formatting)
plt.figure(figsize=(10, 8))
corr = df.corr(numeric_only=True)
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5, square=True, cbar_kws={"shrink": .8})
plt.title("Correlation Matrix of All Features")
plt.show()

# 4. Barplot of Target Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x="Personality", data=df, palette="pastel")
plt.title("Target Class Distribution: Personality")
plt.xticks([0, 1], labels=["Extrovert", "Introvert"])
plt.xlabel("Personality Type")
plt.ylabel("Count")
plt.show()

"""# STEP 5: Feature and Target Definition"""

X = df.drop("Personality", axis=1)
y = df["Personality"]

"""#  STEP 6: Data Scaling"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""# STEP 7: Train/Test Split"""

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=537, stratify=y)

"""# STEP 8: Modeling â€“ Logistic Regression"""

lr = LogisticRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Logistic Regression Report:\n", classification_report(y_test, y_pred_lr))
sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='d', cmap='Blues')
plt.title("LR Confusion Matrix")
plt.show()

"""#  STEP 9: Modeling â€“ Random Forest"""

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Random Forest Report:\n", classification_report(y_test, y_pred_rf))
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Greens')
plt.title("RF Confusion Matrix")
plt.show()

"""# STEP 10: Modeling â€“ Support Vector Machine"""

svm = SVC(kernel='rbf', C=1, gamma='scale')
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)

print("SVM Report:\n", classification_report(y_test, y_pred_svm))
sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='d', cmap='Oranges')
plt.title("SVM Confusion Matrix")
plt.show()

"""# STEP 11: Model Comparison"""

print("LR Accuracy:", accuracy_score(y_test, y_pred_lr))
print("RF Accuracy:", accuracy_score(y_test, y_pred_rf))
print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))

"""# Project Summary: Personality Type Classification Using ML


## Workflow Summary


### 1. Library Imports

Used standard Python libraries:

* `pandas`, `numpy` for data handling
* `matplotlib`, `seaborn` for visualization
* `scikit-learn` for modeling and evaluation

---

### 2. Data Loading & Initial Checks

* Loaded the dataset using `pd.read_csv()`
* Verified there were **no missing values** or **duplicates**
* Target column `Personality` converted to categorical for EDA

---

### 3. Data Cleaning

* Negative zeros in `Time_spent_Alone` replaced with actual zero
* Data types checked and formatted
* No outlier removal was needed due to synthetic structure

---

### 4. Exploratory Data Analysis (EDA)

* **Violin Plots** to compare feature distributions between personality types
* **KDE Plots** for density distribution comparison
* **Correlation Heatmap** to understand feature relationships
* **Class Distribution Plot** to verify label balance (nearly equal)

All visualizations were high-quality and aimed at insight extraction.

---

### 5. Feature Engineering

* Feature matrix `X` and target vector `y` defined
* All features were numeric, so no encoding was needed

---

### 6. Feature Scaling

* Applied `StandardScaler` to normalize input features

---

### 7. Train/Test Split

* Stratified 75/25 split to maintain balanced class distribution
* Used `random_state=42` for reproducibility

---

## Modeling & Evaluation

Three commonly used classification models were implemented and compared:

| Model                        | Description                            |
| ---------------------------- | -------------------------------------- |
| Logistic Regression          | Simple linear model for baseline       |
| Random Forest Classifier     | Ensemble model for robust performance  |
| Support Vector Machine (RBF) | Effective for small to medium datasets |

For each model, we computed:

* **Accuracy**
* **Precision, Recall, F1-Score** via `classification_report`
* **Confusion Matrix** as heatmap

---

## Results Comparison

| Model               | Performance Metric (Test Set) |
| ------------------- | ----------------------------- |
| Logistic Regression | \~ 0.92                       |
| Random Forest       | \~ 0.9504                     |
| SVM (RBF)           | \~ 0.9408                     |


Random Forest generally showed the best balance between accuracy and generalization.

---

# Thank you for taking the time to review my work. I would be very happy if you could upvote! ðŸ˜Š

---



"""

