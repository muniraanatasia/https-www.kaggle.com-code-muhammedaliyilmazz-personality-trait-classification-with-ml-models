# -*- coding: utf-8 -*-
"""Personality Trait Classification with ML Models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/muniraanatasiamhata/personality-trait-classification-with-ml-models.1ff9f1d5-5ab5-4396-8853-fd9b6aecc838.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250630/auto/storage/goog4_request%26X-Goog-Date%3D20250630T021418Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D39481e0237a8a4ccbb1ffdcf6ea77751949df25e6614e0e9fced7e7f3b37a60d0e9570850a0c07a41a49cc72efc777a6bee99d5c5cacc9c3f3016d7720c773dfb1a7fb953ffbef8c353b22ace72f16d1b83d184efb1ad8d0c7aff29a107b6399791e84fcfeea5e5c82a065e422c4ce1380017e952f9c0f38193bb9f08ae85c478590e112aa1dcc3e05d66e7dfc3d86535adfa56cdd47fd7b0887010a9d766da91d269a97c65882eda0868a1b1b7f48ceb7326ec54f495f31fad27739978e9cd8e243cd914608876dde63fb6ae0799bef9c305862c240729e905d629d40d408ad220fb3ce1b2e8cb993611f99e38b1a70b1f0e6c4438035266baa668c8697655e
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
syncoraai_extrovert_vs_introvert_personality_traits_dataset_path = kagglehub.dataset_download('syncoraai/extrovert-vs-introvert-personality-traits-dataset')

print('Data source import complete.')

"""---

## Dataset Summary

**Dataset Name:** Extrovert vs. Introvert Personality Traits Dataset (Synthetic)
**Source:** Generated by Syncoraai (synthetic behavioral data)
**Total Records:** 4,998
**Total Columns:** 8
**Task Type:** Binary Classification
**Target Variable:** `Personality`
**Class Labels:**

* `0` → Extrovert
* `1` → Introvert

The dataset contains information about individuals’ behavioral patterns, habits, and social tendencies. Each row represents one person, and the goal is to classify them as either extroverted or introverted based on observed traits.

---

## Feature Descriptions

| Feature Name                | Data Type | Description                                                                                                           |
| --------------------------- | --------- | --------------------------------------------------------------------------------------------------------------------- |
| `Time_spent_Alone`          | float     | The number of hours a person spends alone per day. Higher values may indicate introverted tendencies.                 |
| `Stage_fear`                | int (0/1) | Indicates whether the person has a fear of public speaking. `1` means they do, which may correlate with introversion. |
| `Social_event_attendance`   | float     | Frequency of attending social events (e.g., parties, gatherings). Higher frequency suggests extroversion.             |
| `Going_outside`             | float     | Represents how often the person goes outdoors for activities or errands.                                              |
| `Drained_after_socializing` | int (0/1) | Whether the person feels emotionally drained after social interaction. `1` = Yes, common in introverts.               |
| `Friends_circle_size`       | float     | Size of the individual’s social circle. A larger number typically correlates with extroversion.                       |
| `Post_frequency`            | float     | Frequency of posting on social media platforms. This can reflect communication preferences and sociability.           |
| `Personality`               | int (0/1) | **Target column.** `0` for Extrovert, `1` for Introvert. This is what we aim to predict.                              |

---

# STEP 1: Import Libraries
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
import warnings
warnings.filterwarnings("ignore")
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# STEP 2: Load the Dataset"""

df = pd.read_csv("/kaggle/input/extrovert-vs-introvert-personality-traits-dataset/Extrovert vs. Introvert Personality Traits Dataset_Syncoraai_Synthetic data.csv")

df.head()

df.tail()

df.info()

df.shape

df.describe()

df.nunique()

"""# STEP 3: Data Cleaning"""

df.isnull().sum()

print(f"Duplicates: {df.duplicated().sum()}")

# Remove negative zeros (if any)
df['Time_spent_Alone'] = df['Time_spent_Alone'].apply(lambda x: 0 if x == -0 else x)

"""# STEP 4: Exploratory Data Analysis (EDA)"""

# Ensure Personality is a category
df['Personality'] = df['Personality'].astype("category")

# 1. Violin plots for feature distribution by Personality
plt.figure(figsize=(18, 12))
for i, col in enumerate(df.drop("Personality", axis=1).columns, 1):
    plt.subplot(3, 3, i)
    sns.violinplot(x="Personality", y=col, data=df, palette="Set3", inner="quartile")
    plt.title(f"{col} Distribution by Personality")
plt.tight_layout()
plt.suptitle("Violin Plots of Features by Personality", y=1.02)
plt.show()

# 2. KDE plots: Density Estimation
plt.figure(figsize=(18, 12))
for i, col in enumerate(df.drop("Personality", axis=1).columns, 1):
    plt.subplot(3, 3, i)
    sns.kdeplot(data=df, x=col, hue="Personality", fill=True, alpha=0.5)
    plt.title(f"KDE Plot: {col}")
plt.tight_layout()
plt.suptitle("KDE Distributions by Personality", y=1.02)
plt.show()

# 3. Correlation Heatmap (Advanced formatting)
plt.figure(figsize=(10, 8))
corr = df.corr(numeric_only=True)
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5, square=True, cbar_kws={"shrink": .8})
plt.title("Correlation Matrix of All Features")
plt.show()

# 4. Barplot of Target Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x="Personality", data=df, palette="pastel")
plt.title("Target Class Distribution: Personality")
plt.xticks([0, 1], labels=["Extrovert", "Introvert"])
plt.xlabel("Personality Type")
plt.ylabel("Count")
plt.show()

"""# STEP 5: Feature and Target Definition"""

X = df.drop("Personality", axis=1)
y = df["Personality"]

"""#  STEP 6: Data Scaling"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""# STEP 7: Train/Test Split"""

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=537, stratify=y)

"""# STEP 8: Modeling – Logistic Regression"""

lr = LogisticRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Logistic Regression Report:\n", classification_report(y_test, y_pred_lr))
sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='d', cmap='Blues')
plt.title("LR Confusion Matrix")
plt.show()

"""#  STEP 9: Modeling – Random Forest"""

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Random Forest Report:\n", classification_report(y_test, y_pred_rf))
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Greens')
plt.title("RF Confusion Matrix")
plt.show()

"""# STEP 10: Modeling – Support Vector Machine"""

svm = SVC(kernel='rbf', C=1, gamma='scale')
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)

print("SVM Report:\n", classification_report(y_test, y_pred_svm))
sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='d', cmap='Oranges')
plt.title("SVM Confusion Matrix")
plt.show()

"""# STEP 11: Model Comparison"""

print("LR Accuracy:", accuracy_score(y_test, y_pred_lr))
print("RF Accuracy:", accuracy_score(y_test, y_pred_rf))
print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))

"""# Project Summary: Personality Type Classification Using ML


## Workflow Summary


### 1. Library Imports

Used standard Python libraries:

* `pandas`, `numpy` for data handling
* `matplotlib`, `seaborn` for visualization
* `scikit-learn` for modeling and evaluation

---

### 2. Data Loading & Initial Checks

* Loaded the dataset using `pd.read_csv()`
* Verified there were **no missing values** or **duplicates**
* Target column `Personality` converted to categorical for EDA

---

### 3. Data Cleaning

* Negative zeros in `Time_spent_Alone` replaced with actual zero
* Data types checked and formatted
* No outlier removal was needed due to synthetic structure

---

### 4. Exploratory Data Analysis (EDA)

* **Violin Plots** to compare feature distributions between personality types
* **KDE Plots** for density distribution comparison
* **Correlation Heatmap** to understand feature relationships
* **Class Distribution Plot** to verify label balance (nearly equal)

All visualizations were high-quality and aimed at insight extraction.

---

### 5. Feature Engineering

* Feature matrix `X` and target vector `y` defined
* All features were numeric, so no encoding was needed

---

### 6. Feature Scaling

* Applied `StandardScaler` to normalize input features

---

### 7. Train/Test Split

* Stratified 75/25 split to maintain balanced class distribution
* Used `random_state=42` for reproducibility

---

## Modeling & Evaluation

Three commonly used classification models were implemented and compared:

| Model                        | Description                            |
| ---------------------------- | -------------------------------------- |
| Logistic Regression          | Simple linear model for baseline       |
| Random Forest Classifier     | Ensemble model for robust performance  |
| Support Vector Machine (RBF) | Effective for small to medium datasets |

For each model, we computed:

* **Accuracy**
* **Precision, Recall, F1-Score** via `classification_report`
* **Confusion Matrix** as heatmap

---

## Results Comparison

| Model               | Performance Metric (Test Set) |
| ------------------- | ----------------------------- |
| Logistic Regression | \~ 0.92                       |
| Random Forest       | \~ 0.9504                     |
| SVM (RBF)           | \~ 0.9408                     |


Random Forest generally showed the best balance between accuracy and generalization.

---

# Thank you for taking the time to review my work. I would be very happy if you could upvote! 😊

---



"""

